{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVkCC1iri2SN"
      },
      "source": [
        "## HW 4: Policy gradient\n",
        "_Reference: based on Practical RL course by YSDA_\n",
        "\n",
        "In this notebook you have to master Policy gradient Q-learning and apply it to familiar (and not so familiar) RL problems once again.\n",
        "\n",
        "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
        "\n",
        "\n",
        "In the end of the notebook, please, copy the functions you have implemented to the template file and submit it to the Contest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7UYczVTli2Sb"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "XPKYrIlai2Sf",
        "outputId": "2e044ee7-3baa-4bd7-a214-23b7225a88b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f14aefad550>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAF7CAYAAAD4/3BBAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKDhJREFUeJzt3X9wVFWe//9XJ6HDj9AdAySdSIIoDBAh6AKGXkeXGTIEiK6sccsfLMRZCkomsQbiMJgZR8TZj3Fxa/01ClWf3RW3PjKMTImujMDEIGEdA2KGLL8kK3zYDQ7phJFNN0QJJH0+f/jlfqcVIR1C9wl5PqpuVfqe07ff9xR1+8W95952GWOMAAAALJIQ7wIAAAC+ioACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKwT14Dy0ksv6brrrlP//v2Vn5+vDz/8MJ7lAAAAS8QtoPzqV79SeXm5VqxYod///veaOHGiCgsL1dLSEq+SAACAJVzx+rHA/Px8TZkyRb/4xS8kSeFwWNnZ2Xr44Yf16KOPxqMkAABgiaR4fOjZs2dVV1eniooKZ11CQoIKCgpUW1v7tf7t7e1qb293XofDYZ08eVJDhgyRy+WKSc0AAODyGGN06tQpZWVlKSHh4hdx4hJQ/vjHP6qzs1MZGRkR6zMyMnTo0KGv9a+srNTKlStjVR4AALiCjh07puHDh1+0T1wCSrQqKipUXl7uvA4Gg8rJydGxY8fk8XjiWBkAAOiqUCik7OxsDR48+JJ94xJQhg4dqsTERDU3N0esb25uls/n+1r/5ORkJScnf229x+MhoAAA0Mt0ZXpGXO7icbvdmjRpkqqrq5114XBY1dXV8vv98SgJAABYJG6XeMrLy1VSUqLJkyfrlltu0XPPPae2tjZ9//vfj1dJAADAEnELKPfee69OnDihxx9/XIFAQDfddJO2bNnytYmzAACg74nbc1AuRygUktfrVTAYZA4KAAC9RDTf3/wWDwAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdXo8oDzxxBNyuVwRy9ixY532M2fOqLS0VEOGDFFKSoqKi4vV3Nzc02UAAIBe7IqcQbnxxhvV1NTkLO+//77TtnTpUr399tvasGGDampqdPz4cd19991XogwAANBLJV2RjSYlyefzfW19MBjUP//zP2vdunX67ne/K0l65ZVXNG7cOO3cuVNTp069EuUAAIBe5oqcQfnkk0+UlZWl66+/XnPnzlVjY6Mkqa6uTufOnVNBQYHTd+zYscrJyVFtbe03bq+9vV2hUChiAQAAV68eDyj5+flau3attmzZotWrV+vo0aO67bbbdOrUKQUCAbndbqWmpka8JyMjQ4FA4Bu3WVlZKa/X6yzZ2dk9XTYAALBIj1/imTVrlvN3Xl6e8vPzNWLECL3++usaMGBAt7ZZUVGh8vJy53UoFCKkAABwFbvitxmnpqbqW9/6lg4fPiyfz6ezZ8+qtbU1ok9zc/MF56ycl5ycLI/HE7EAAICr1xUPKKdPn9aRI0eUmZmpSZMmqV+/fqqurnbaGxoa1NjYKL/ff6VLAQAAvUSPX+L50Y9+pDvvvFMjRozQ8ePHtWLFCiUmJur++++X1+vVggULVF5errS0NHk8Hj388MPy+/3cwQMAABw9HlA+/fRT3X///frss880bNgwffvb39bOnTs1bNgwSdKzzz6rhIQEFRcXq729XYWFhXr55Zd7ugwAANCLuYwxJt5FRCsUCsnr9SoYDDIfBQCAXiKa729+iwcAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYJ2oA8qOHTt05513KisrSy6XS2+++WZEuzFGjz/+uDIzMzVgwAAVFBTok08+iehz8uRJzZ07Vx6PR6mpqVqwYIFOnz59WTsCAACuHlEHlLa2Nk2cOFEvvfTSBdtXrVqlF154QWvWrNGuXbs0aNAgFRYW6syZM06fuXPn6sCBA6qqqtKmTZu0Y8cOLVq0qPt7AQAAriouY4zp9ptdLm3cuFFz5syR9OXZk6ysLD3yyCP60Y9+JEkKBoPKyMjQ2rVrdd999+njjz9Wbm6udu/ercmTJ0uStmzZotmzZ+vTTz9VVlbWJT83FArJ6/UqGAzK4/F0t3wAABBD0Xx/9+gclKNHjyoQCKigoMBZ5/V6lZ+fr9raWklSbW2tUlNTnXAiSQUFBUpISNCuXbsuuN329naFQqGIBQAAXL16NKAEAgFJUkZGRsT6jIwMpy0QCCg9PT2iPSkpSWlpaU6fr6qsrJTX63WW7OzsniwbAABYplfcxVNRUaFgMOgsx44di3dJAADgCurRgOLz+SRJzc3NEeubm5udNp/Pp5aWloj2jo4OnTx50unzVcnJyfJ4PBELAAC4evVoQBk5cqR8Pp+qq6uddaFQSLt27ZLf75ck+f1+tba2qq6uzumzbds2hcNh5efn92Q5AACgl0qK9g2nT5/W4cOHnddHjx5VfX290tLSlJOToyVLlujv/u7vNHr0aI0cOVI/+9nPlJWV5dzpM27cOM2cOVMLFy7UmjVrdO7cOZWVlem+++7r0h08AADg6hd1QPnoo4/0ne98x3ldXl4uSSopKdHatWv14x//WG1tbVq0aJFaW1v17W9/W1u2bFH//v2d97z22msqKyvT9OnTlZCQoOLiYr3wwgs9sDsAAOBqcFnPQYkXnoMCAEDvE7fnoAAAAPQEAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOtEHVB27NihO++8U1lZWXK5XHrzzTcj2h988EG5XK6IZebMmRF9Tp48qblz58rj8Sg1NVULFizQ6dOnL2tHAADA1SPqgNLW1qaJEyfqpZde+sY+M2fOVFNTk7P88pe/jGifO3euDhw4oKqqKm3atEk7duzQokWLoq8eAABclZKifcOsWbM0a9asi/ZJTk6Wz+e7YNvHH3+sLVu2aPfu3Zo8ebIk6cUXX9Ts2bP1D//wD8rKyoq2JAAAcJW5InNQtm/frvT0dI0ZM0aLFy/WZ5995rTV1tYqNTXVCSeSVFBQoISEBO3ateuC22tvb1coFIpYAADA1avHA8rMmTP1r//6r6qurtbf//3fq6amRrNmzVJnZ6ckKRAIKD09PeI9SUlJSktLUyAQuOA2Kysr5fV6nSU7O7unywYAABaJ+hLPpdx3333O3xMmTFBeXp5uuOEGbd++XdOnT+/WNisqKlReXu68DoVChBQAAK5iV/w24+uvv15Dhw7V4cOHJUk+n08tLS0RfTo6OnTy5MlvnLeSnJwsj8cTsQAAgKvXFQ8on376qT777DNlZmZKkvx+v1pbW1VXV+f02bZtm8LhsPLz8690OQAAoBeI+hLP6dOnnbMhknT06FHV19crLS1NaWlpWrlypYqLi+Xz+XTkyBH9+Mc/1qhRo1RYWChJGjdunGbOnKmFCxdqzZo1OnfunMrKynTfffdxBw8AAJAkuYwxJpo3bN++Xd/5zne+tr6kpESrV6/WnDlztGfPHrW2tiorK0szZszQz3/+c2VkZDh9T548qbKyMr399ttKSEhQcXGxXnjhBaWkpHSphlAoJK/Xq2AwyOUeAAB6iWi+v6MOKDYgoAAA0PtE8/3Nb/EAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHWi/rFAAOiuznNndOTd/33RPi6XS6Nm/ECuBP7/BPRlBBQAMWPCnQo27rt4J1eCjAnLxQleoE/jCADAOibcGe8SAMQZAQWAdQgoAAgoAKxjTDjeJQCIMwIKAPuECShAX0dAAWAdY7jEA/R1BBQA1mEOCgACCgDrGC7xAH0eAQWAdTiDAoCAAsAyhjMoAAgoAOzDJFkABBQA1uEMCgACCgDrMAcFAAEFgHV4kiwAAgoA+zAHBejzCCgArMMlHgAEFADWYZIsAAIKAOtwBgUAAQWAdTiDAiCqgFJZWakpU6Zo8ODBSk9P15w5c9TQ0BDR58yZMyotLdWQIUOUkpKi4uJiNTc3R/RpbGxUUVGRBg4cqPT0dC1btkwdHR2XvzcAej9DQAEQZUCpqalRaWmpdu7cqaqqKp07d04zZsxQW1ub02fp0qV6++23tWHDBtXU1Oj48eO6++67nfbOzk4VFRXp7Nmz+uCDD/Tqq69q7dq1evzxx3turwD0btzFA/R5LmOM6e6bT5w4ofT0dNXU1Oj2229XMBjUsGHDtG7dOt1zzz2SpEOHDmncuHGqra3V1KlTtXnzZt1xxx06fvy4MjIyJElr1qzR8uXLdeLECbnd7kt+bigUktfrVTAYlMfj6W75AGKso71Ne9YuvUQvl0ZOe1BDx/hjUhOA2Inm+/uy5qAEg0FJUlpamiSprq5O586dU0FBgdNn7NixysnJUW1trSSptrZWEyZMcMKJJBUWFioUCunAgQMX/Jz29naFQqGIBcDViwe1Aeh2QAmHw1qyZIluvfVWjR8/XpIUCATkdruVmpoa0TcjI0OBQMDp86fh5Hz7+bYLqayslNfrdZbs7Ozulg2gF+AuHgDdDiilpaXav3+/1q9f35P1XFBFRYWCwaCzHDt27Ip/JoD4IaAA6FZAKSsr06ZNm/Tee+9p+PDhznqfz6ezZ8+qtbU1on9zc7N8Pp/T56t39Zx/fb7PVyUnJ8vj8UQsAHojlxLdAy7Z69yZUzGoBYDNogooxhiVlZVp48aN2rZtm0aOHBnRPmnSJPXr10/V1dXOuoaGBjU2Nsrv/3LCm9/v1759+9TS0uL0qaqqksfjUW5u7uXsCwDLJST20zU3TL5EL6OTn+yKST0A7JUUTefS0lKtW7dOb731lgYPHuzMGfF6vRowYIC8Xq8WLFig8vJypaWlyePx6OGHH5bf79fUqVMlSTNmzFBubq7mzZunVatWKRAI6LHHHlNpaamSk5N7fg8BWMXl4vmQAC4tqoCyevVqSdK0adMi1r/yyit68MEHJUnPPvusEhISVFxcrPb2dhUWFurll192+iYmJmrTpk1avHix/H6/Bg0apJKSEj355JOXtycAegVXQmK8SwDQC1zWc1DiheegAL1TuPOcPt21Uc373r1ov2TPMOXd/79iVBWAWInZc1AAIGoJHHYAXBpHCgAxxRwUAF3BkQJADLkIKAC6hCMFgJhikiyAriCgAIgpF3NQAHQBRwoAseXiDAqASyOgAIghF2dQAHQJRwoAMeNycRcPgK7hSAEgpjiDAqArOFIAiC3moADoAgIKgJjiDAqAruBIASCmmIMCoCs4UgCIIRcPagPQJQQUADHFJR4AXcGRAkDscJsxgC7iSAEgtggoALqAIwWAGOJJsgC6hiMFgJhikiyAriCgAIgp5qAA6AqOFABiioACoCs4UgCILeagAOgCjhQAYsblcnX5DIox5gpXA8BmBBQA9jFGxoTjXQWAOCKgALATZ1CAPo2AAsA6RpIJcwYF6MsIKAAsZCQu8QB9GgEFgH2MmIMC9HEEFAAWYpIs0NcRUADYiTkoQJ9GQAFgH24zBvo8AgoAK5lwZ7xLABBHUQWUyspKTZkyRYMHD1Z6errmzJmjhoaGiD7Tpk37/54W+f8vDz30UESfxsZGFRUVaeDAgUpPT9eyZcvU0dFx+XsD4KpgxCRZoK9LiqZzTU2NSktLNWXKFHV0dOgnP/mJZsyYoYMHD2rQoEFOv4ULF+rJJ590Xg8cOND5u7OzU0VFRfL5fPrggw/U1NSk+fPnq1+/fnrqqad6YJcA9H7cZgz0dVEFlC1btkS8Xrt2rdLT01VXV6fbb7/dWT9w4ED5fL4LbuO3v/2tDh48qHfffVcZGRm66aab9POf/1zLly/XE088Ibfb3Y3dAHBVMZIJ8yRZoC+7rDkowWBQkpSWlhax/rXXXtPQoUM1fvx4VVRU6PPPP3faamtrNWHCBGVkZDjrCgsLFQqFdODAgQt+Tnt7u0KhUMQC4GpmJMMcFKAvi+oMyp8Kh8NasmSJbr31Vo0fP95Z/8ADD2jEiBHKysrS3r17tXz5cjU0NOiNN96QJAUCgYhwIsl5HQgELvhZlZWVWrlyZXdLBdAL8WvGQN/W7YBSWlqq/fv36/33349Yv2jRIufvCRMmKDMzU9OnT9eRI0d0ww03dOuzKioqVF5e7rwOhULKzs7uXuEAegV+iwfo27p1iaesrEybNm3Se++9p+HDh1+0b35+viTp8OHDkiSfz6fm5uaIPudff9O8leTkZHk8nogFwNXL8BwUoM+LKqAYY1RWVqaNGzdq27ZtGjly5CXfU19fL0nKzMyUJPn9fu3bt08tLS1On6qqKnk8HuXm5kZTDoCrGGdQgL4tqks8paWlWrdund566y0NHjzYmTPi9Xo1YMAAHTlyROvWrdPs2bM1ZMgQ7d27V0uXLtXtt9+uvLw8SdKMGTOUm5urefPmadWqVQoEAnrsscdUWlqq5OTknt9DAL0Qk2SBvi6qMyirV69WMBjUtGnTlJmZ6Sy/+tWvJElut1vvvvuuZsyYobFjx+qRRx5RcXGx3n77bWcbiYmJ2rRpkxITE+X3+/U3f/M3mj9/fsRzUwD0cfyaMdDnRXUG5VKz6rOzs1VTU3PJ7YwYMULvvPNONB8NoK8hoAB9Gr/FA8BChge1AX0cAQWAlQxzUIA+jYACwD7GfLkA6LMIKACsY8RtxkBfR0ABYCXu4gH6NgIKgJhyp1wjz7VjL9qns71N/3P09zGqCICNCCgAYsuVKFei+5LdTGdHDIoBYCsCCoCYcklyuVzxLgOA5QgoAGLL5ZJcHHoAXBxHCQAxxxkUAJdCQAEQUy6XS64EDj0ALo6jBIAYc4lDD4BL4SgBILZcLimBSzwALo6AAiDmXEySBXAJHCUAxJbLRUABcEkcJQDElEvcZgzg0jhKAIgtF7cZA7g0AgqAGOMMCoBL4ygBILZcLs6gALikpHgXAKB36ei4vB/x6+zslOlCP2PCl/VZCQkJSuCBcECvRUABEJWbbrpJDQ0N3X7/gOQkLZh1sx4oGH/RfhvffFM/veOH3f6c9evXq7i4uNvvBxBfBBQAUens7LysMxvnEqSOzku/34TNZX1OOBzu9nsBxB8BBUBMGWPUGf7yIs9nZzMV7BimTiWpf0Kbhrkb1T/hizhXCMAGBBQAMWWMFA4b/d/P83TszDh9ER4ko0Qludr16ZkxutlTRUgBwF08AGLM5dKx9lx98vlkfR72yihJkksdpr9aO3z6oPVudZrEeFcJIM4IKABiypd5o2657WGFv+EEbnt4oN7/n3tiXBUA2xBQAMTcxZ+DwjNSABBQAACAhQgoAADAOgQUADH1hz8c0PbqX8ilCz+nJMl1Vv7Ut2JcFQDbRBVQVq9erby8PHk8Hnk8Hvn9fm3evNlpP3PmjEpLSzVkyBClpKSouLhYzc3NEdtobGxUUVGRBg4cqPT0dC1btuyyH50NoPfo7OxQhqtO1w+oV3JCm1zqlGSUqLMalPg/ui11g9wJZ+JdJoA4i+o5KMOHD9fTTz+t0aNHyxijV199VXfddZf27NmjG2+8UUuXLtVvfvMbbdiwQV6vV2VlZbr77rv1u9/9TtKXT6AsKiqSz+fTBx98oKamJs2fP1/9+vXTU089dUV2EIB9jhw/qYN7/o9azubo5LlMdZp+GpAYUpb7iLYktkmSDjX+Mc5VAognlzGmK7/b9Y3S0tL0zDPP6J577tGwYcO0bt063XPPl7cIHjp0SOPGjVNtba2mTp2qzZs364477tDx48eVkZEhSVqzZo2WL1+uEydOyO12d+kzQ6GQvF6vHnzwwS6/B0DPeP3119Xa2hrvMi6poKBA119/fbzLAPAnzp49q7Vr1yoYDMrj8Vy0b7efJNvZ2akNGzaora1Nfr9fdXV1OnfunAoKCpw+Y8eOVU5OjhNQamtrNWHCBCecSFJhYaEWL16sAwcO6Oabb77gZ7W3t6u9vd15HQqFJEnz5s1TSkpKd3cBQDds3bq1VwSU6dOn67vf/W68ywDwJ06fPq21a9d2qW/UAWXfvn3y+/06c+aMUlJStHHjRuXm5qq+vl5ut1upqakR/TMyMhQIBCRJgUAgIpycbz/f9k0qKyu1cuXKr62fPHnyJRMYgJ41YMCAeJfQJTfccINuueWWeJcB4E+cP8HQFVHfxTNmzBjV19dr165dWrx4sUpKSnTw4MFoNxOViooKBYNBZzl27NgV/TwAABBfUZ9BcbvdGjVqlCRp0qRJ2r17t55//nnde++9Onv2rFpbWyPOojQ3N8vn80mSfD6fPvzww4jtnb/L53yfC0lOTlZycnK0pQIAgF7qsp+DEg6H1d7erkmTJqlfv36qrq522hoaGtTY2Ci/3y9J8vv92rdvn1paWpw+VVVV8ng8ys3NvdxSAADAVSKqMygVFRWaNWuWcnJydOrUKa1bt07bt2/X1q1b5fV6tWDBApWXlystLU0ej0cPP/yw/H6/pk6dKkmaMWOGcnNzNW/ePK1atUqBQECPPfaYSktLOUMCAAAcUQWUlpYWzZ8/X01NTfJ6vcrLy9PWrVv1ve99T5L07LPPKiEhQcXFxWpvb1dhYaFefvll5/2JiYnatGmTFi9eLL/fr0GDBqmkpERPPvlkz+4VAADo1S77OSjxcP45KF25jxpAzxo3bpwOHToU7zIu6fXXX9df//Vfx7sMAH8imu9vfosHAABYh4ACAACsQ0ABAADWIaAAAADrdPu3eAD0TQUFBRo7dmy8y7ika6+9Nt4lALgMBBQAUXnxxRfjXQKAPoBLPAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHWiCiirV69WXl6ePB6PPB6P/H6/Nm/e7LRPmzZNLpcrYnnooYcittHY2KiioiINHDhQ6enpWrZsmTo6OnpmbwAAwFUhKZrOw4cP19NPP63Ro0fLGKNXX31Vd911l/bs2aMbb7xRkrRw4UI9+eSTznsGDhzo/N3Z2amioiL5fD598MEHampq0vz589WvXz899dRTPbRLAACgt3MZY8zlbCAtLU3PPPOMFixYoGnTpummm27Sc889d8G+mzdv1h133KHjx48rIyNDkrRmzRotX75cJ06ckNvt7tJnhkIheb1eBYNBeTyeyykfAADESDTf392eg9LZ2an169erra1Nfr/fWf/aa69p6NChGj9+vCoqKvT55587bbW1tZowYYITTiSpsLBQoVBIBw4c+MbPam9vVygUilgAAMDVK6pLPJK0b98++f1+nTlzRikpKdq4caNyc3MlSQ888IBGjBihrKws7d27V8uXL1dDQ4PeeOMNSVIgEIgIJ5Kc14FA4Bs/s7KyUitXroy2VAAA0EtFHVDGjBmj+vp6BYNB/frXv1ZJSYlqamqUm5urRYsWOf0mTJigzMxMTZ8+XUeOHNENN9zQ7SIrKipUXl7uvA6FQsrOzu729gAAgN2ivsTjdrs1atQoTZo0SZWVlZo4caKef/75C/bNz8+XJB0+fFiS5PP51NzcHNHn/Gufz/eNn5mcnOzcOXR+AQAAV6/Lfg5KOBxWe3v7Bdvq6+slSZmZmZIkv9+vffv2qaWlxelTVVUlj8fjXCYCAACI6hJPRUWFZs2apZycHJ06dUrr1q3T9u3btXXrVh05ckTr1q3T7NmzNWTIEO3du1dLly7V7bffrry8PEnSjBkzlJubq3nz5mnVqlUKBAJ67LHHVFpaquTk5CuygwAAoPeJKqC0tLRo/vz5ampqktfrVV5enrZu3arvfe97OnbsmN59910999xzamtrU3Z2toqLi/XYY485709MTNSmTZu0ePFi+f1+DRo0SCUlJRHPTQEAALjs56DEA89BAQCg94nJc1AAAACuFAIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCdpHgX0B3GGElSKBSKcyUAAKCrzn9vn/8ev5heGVBOnTolScrOzo5zJQAAIFqnTp2S1+u9aB+X6UqMsUw4HFZDQ4Nyc3N17NgxeTyeeJfUa4VCIWVnZzOOPYCx7DmMZc9gHHsOY9kzjDE6deqUsrKylJBw8VkmvfIMSkJCgq699lpJksfj4R9LD2Acew5j2XMYy57BOPYcxvLyXerMyXlMkgUAANYhoAAAAOv02oCSnJysFStWKDk5Od6l9GqMY89hLHsOY9kzGMeew1jGXq+cJAsAAK5uvfYMCgAAuHoRUAAAgHUIKAAAwDoEFAAAYJ1eGVBeeuklXXfdderfv7/y8/P14Ycfxrsk6+zYsUN33nmnsrKy5HK59Oabb0a0G2P0+OOPKzMzUwMGDFBBQYE++eSTiD4nT57U3Llz5fF4lJqaqgULFuj06dMx3Iv4q6ys1JQpUzR48GClp6drzpw5amhoiOhz5swZlZaWasiQIUpJSVFxcbGam5sj+jQ2NqqoqEgDBw5Uenq6li1bpo6OjljuSlytXr1aeXl5zkOu/H6/Nm/e7LQzht339NNPy+VyacmSJc46xrNrnnjiCblcrohl7NixTjvjGGeml1m/fr1xu93mX/7lX8yBAwfMwoULTWpqqmlubo53aVZ55513zE9/+lPzxhtvGElm48aNEe1PP/208Xq95s033zT/8R//Yf7yL//SjBw50nzxxRdOn5kzZ5qJEyeanTt3mn//9383o0aNMvfff3+M9yS+CgsLzSuvvGL2799v6uvrzezZs01OTo45ffq00+ehhx4y2dnZprq62nz00Udm6tSp5s///M+d9o6ODjN+/HhTUFBg9uzZY9555x0zdOhQU1FREY9diot/+7d/M7/5zW/Mf/7nf5qGhgbzk5/8xPTr18/s37/fGMMYdteHH35orrvuOpOXl2d++MMfOusZz65ZsWKFufHGG01TU5OznDhxwmlnHOOr1wWUW265xZSWljqvOzs7TVZWlqmsrIxjVXb7akAJh8PG5/OZZ555xlnX2tpqkpOTzS9/+UtjjDEHDx40kszu3budPps3bzYul8v84Q9/iFnttmlpaTGSTE1NjTHmy3Hr16+f2bBhg9Pn448/NpJMbW2tMebLsJiQkGACgYDTZ/Xq1cbj8Zj29vbY7oBFrrnmGvNP//RPjGE3nTp1yowePdpUVVWZv/iLv3ACCuPZdStWrDATJ068YBvjGH+96hLP2bNnVVdXp4KCAmddQkKCCgoKVFtbG8fKepejR48qEAhEjKPX61V+fr4zjrW1tUpNTdXkyZOdPgUFBUpISNCuXbtiXrMtgsGgJCktLU2SVFdXp3PnzkWM5dixY5WTkxMxlhMmTFBGRobTp7CwUKFQSAcOHIhh9Xbo7OzU+vXr1dbWJr/fzxh2U2lpqYqKiiLGTeLfZLQ++eQTZWVl6frrr9fcuXPV2NgoiXG0Qa/6scA//vGP6uzsjPjHIEkZGRk6dOhQnKrqfQKBgCRdcBzPtwUCAaWnp0e0JyUlKS0tzenT14TDYS1ZskS33nqrxo8fL+nLcXK73UpNTY3o+9WxvNBYn2/rK/bt2ye/368zZ84oJSVFGzduVG5ururr6xnDKK1fv16///3vtXv37q+18W+y6/Lz87V27VqNGTNGTU1NWrlypW677Tbt37+fcbRArwooQDyVlpZq//79ev/99+NdSq80ZswY1dfXKxgM6te//rVKSkpUU1MT77J6nWPHjumHP/yhqqqq1L9//3iX06vNmjXL+TsvL0/5+fkaMWKEXn/9dQ0YMCCOlUHqZXfxDB06VImJiV+bRd3c3Cyfzxenqnqf82N1sXH0+XxqaWmJaO/o6NDJkyf75FiXlZVp06ZNeu+99zR8+HBnvc/n09mzZ9Xa2hrR/6tjeaGxPt/WV7jdbo0aNUqTJk1SZWWlJk6cqOeff54xjFJdXZ1aWlr0Z3/2Z0pKSlJSUpJqamr0wgsvKCkpSRkZGYxnN6Wmpupb3/qWDh8+zL9LC/SqgOJ2uzVp0iRVV1c768LhsKqrq+X3++NYWe8ycuRI+Xy+iHEMhULatWuXM45+v1+tra2qq6tz+mzbtk3hcFj5+fkxrzlejDEqKyvTxo0btW3bNo0cOTKifdKkSerXr1/EWDY0NKixsTFiLPft2xcR+KqqquTxeJSbmxubHbFQOBxWe3s7Yxil6dOna9++faqvr3eWyZMna+7cuc7fjGf3nD59WkeOHFFmZib/Lm0Q71m60Vq/fr1JTk42a9euNQcPHjSLFi0yqampEbOo8eUM/z179pg9e/YYSeYf//EfzZ49e8x///d/G2O+vM04NTXVvPXWW2bv3r3mrrvuuuBtxjfffLPZtWuXef/9983o0aP73G3GixcvNl6v12zfvj3iVsTPP//c6fPQQw+ZnJwcs23bNvPRRx8Zv99v/H6/037+VsQZM2aY+vp6s2XLFjNs2LA+dSvio48+ampqaszRo0fN3r17zaOPPmpcLpf57W9/a4xhDC/Xn97FYwzj2VWPPPKI2b59uzl69Kj53e9+ZwoKCszQoUNNS0uLMYZxjLdeF1CMMebFF180OTk5xu12m1tuucXs3Lkz3iVZ57333jOSvraUlJQYY7681fhnP/uZycjIMMnJyWb69OmmoaEhYhufffaZuf/++01KSorxeDzm+9//vjl16lQc9iZ+LjSGkswrr7zi9Pniiy/MD37wA3PNNdeYgQMHmr/6q78yTU1NEdv5r//6LzNr1iwzYMAAM3ToUPPII4+Yc+fOxXhv4udv//ZvzYgRI4zb7TbDhg0z06dPd8KJMYzh5fpqQGE8u+bee+81mZmZxu12m2uvvdbce++95vDhw0474xhfLmOMic+5GwAAgAvrVXNQAABA30BAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1/h/rtaF2wGQc8wAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75eHkuwTi2Si"
      },
      "source": [
        "# Building the network for Policy Gradient (REINFORCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_TFCmsWi2Sj"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sY2THBWfi2Sl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8_pYr7PZi2Sn"
      },
      "outputs": [],
      "source": [
        "# Build a simple neural network that predicts policy logits.\n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(4, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 2)\n",
        ")\n",
        "assert model is not None, \"model is not defined\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "example_states_batch.shape: (5, 4)\n",
            "example_logits.shape: torch.Size([5, 2])\n"
          ]
        }
      ],
      "source": [
        "# do not change the code block below\n",
        "batch_size_for_test = 5\n",
        "example_states_batch = np.array([env.reset()[0] for _ in range(5)])\n",
        "print(f\"example_states_batch.shape: {example_states_batch.shape}\")\n",
        "assert example_states_batch.shape == (batch_size_for_test, state_dim[0])\n",
        "\n",
        "example_logits = model(torch.from_numpy(example_states_batch))\n",
        "print(f\"example_logits.shape: {example_logits.shape}\")\n",
        "assert example_logits.shape == (batch_size_for_test, n_actions)\n",
        "# do not change the code block above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y80qbQFi2Sq"
      },
      "source": [
        "#### Predicting the action probas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12PjRu0mi2Sr"
      },
      "source": [
        "Note: **output value of this function is not a torch tensor, it's a numpy array.**\n",
        "\n",
        "So, here gradient calculation is not needed.\n",
        "\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "\n",
        "Also, `.detach()` can be used instead, but there is a difference:\n",
        "\n",
        "* With `.detach()` computational graph is built but then disconnected from a particular tensor, so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "* In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d5B5JuXCi2St"
      },
      "outputs": [],
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\"\n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :param model: torch model\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "\n",
        "    # YOUR CODE GOES HERE\n",
        "    with torch.no_grad():\n",
        "\n",
        "        states_tensor = torch.FloatTensor(states)\n",
        "        logits = model(states_tensor)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        probs = probs.numpy()\n",
        "        \n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Obkl_jCii2Sv"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset()[0] for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6AYf8gi2Sw"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8LOUUvnki2Sx"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\"\n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s, info = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, truncated, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5sdENWJAi2Sz"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG5hLg-3i2S0"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "To work with sequential environments we need the cumulative discounted reward for known for every state. To compute it we can **roll back** from the end of the session to the beginning and compute the discounted cumulative reward as following:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AoWX9gvai2S0"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session\n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "\n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    # YOUR CODE GOES HERE\n",
        "    \n",
        "    cumulative_rewards = []\n",
        "    G = 0\n",
        "    \n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        cumulative_rewards.append(G)\n",
        "    \n",
        "    \n",
        "    cumulative_rewards = list(reversed(cumulative_rewards))\n",
        "\n",
        "    assert cumulative_rewards is not None, \"cumulative_rewards is not defined\"\n",
        "\n",
        "    return cumulative_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DX39wcUi2S3",
        "outputId": "9916590d-b093-4c5b-dd84-4ed9ed4b2ba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ]
        }
      ],
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evLt5DJji2S_"
      },
      "source": [
        "### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient.\n",
        "\n",
        "Final loss should also include the entropy regularization term $H(\\pi_\\theta (a_i \\mid s_i))$ to enforce the exploration:\n",
        "\n",
        "$$\n",
        "L = -\\hat J(\\theta) - \\lambda H(\\pi_\\theta (a_i \\mid s_i)),\n",
        "$$\n",
        "where $\\lambda$ is the `entropy_coef`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function might be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_hLjxTVLi2TB"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_loss(logits, actions, rewards, n_actions=n_actions, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Compute the loss for the REINFORCE algorithm.\n",
        "    \"\"\"\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    assert probs is not None, \"probs is not defined\"\n",
        "\n",
        "    log_probs = torch.log_softmax(logits, dim=-1)\n",
        "    assert log_probs is not None, \"log_probs is not defined\"\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = log_probs[range(len(actions)), actions] # [batch,]\n",
        "    assert log_probs_for_actions is not None, \"log_probs_for_actions is not defined\"\n",
        "    J_hat = torch.mean(log_probs_for_actions * cumulative_returns)  # a number\n",
        "    assert J_hat is not None, \"J_hat is not defined\"\n",
        "    \n",
        "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
        "    entropy = -torch.sum(probs * log_probs, dim=-1)\n",
        "    assert entropy is not None, \"entropy is not defined\"\n",
        "    mean_entropy = torch.mean(entropy)\n",
        "    loss = -J_hat - entropy_coef * mean_entropy\n",
        "    assert loss is not None, \"loss is not defined\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1C8ZSizji2TD"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.999, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    logits = model(states)\n",
        "    # cast everything into torch tensors\n",
        "    loss = get_loss(logits, actions, rewards, n_actions=n_actions, gamma=gamma, entropy_coef=entropy_coef)\n",
        "    # Gradient descent step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-WWsbl5i2TE"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckHj5sXBi2TE",
        "outputId": "017d3bcd-0d01-4632-ed1c-60642792cddf",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_31820/1492844302.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  states = torch.tensor(states, dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean reward:25.810\n",
            "mean reward:35.230\n",
            "mean reward:61.490\n",
            "mean reward:147.790\n",
            "mean reward:248.820\n",
            "mean reward:633.670\n",
            "mean reward:110.940\n",
            "mean reward:220.500\n",
            "mean reward:230.690\n",
            "mean reward:813.290\n",
            "You Win!\n"
          ]
        }
      ],
      "source": [
        "for i in range(500):\n",
        "    rewards = [train_on_session(*generate_session(env), entropy_coef=1e-3) for _ in range(100)]  # generate new sessions\n",
        "\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 800:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg__sQeti2TF"
      },
      "source": [
        "### Watch the video of your results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import gymnasium as gym\n",
        "# from gymnasium.utils.save_video import save_video\n",
        "\n",
        "# env_for_video = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
        "# n_actions = env_for_video.action_space.n\n",
        "\n",
        "# episode_index = 0\n",
        "# step_starting_index = 0\n",
        "\n",
        "# obs, info = env_for_video.reset()\n",
        "\n",
        "# for step_index in range(800):\n",
        "#     probs = predict_probs(np.array([obs]))[0]\n",
        "#     action = np.random.choice(n_actions, p=probs)\n",
        "\n",
        "#     obs, reward, terminated, truncated, info = env_for_video.step(action)\n",
        "#     done = terminated or truncated\n",
        "\n",
        "#     if done or step_index == 799:\n",
        "#         # env_for_video.render() now returns the LIST of frames accumulated so far\n",
        "#         frames = env_for_video.render()\n",
        "#         os.makedirs(\"videos\", exist_ok=True)\n",
        "#         save_video(\n",
        "#             frames, \"videos\",\n",
        "#             fps=env_for_video.metadata.get(\"render_fps\", 30),\n",
        "#             step_starting_index=step_starting_index,\n",
        "#             episode_index=episode_index,\n",
        "#         )\n",
        "#         episode_index += 1\n",
        "#         step_starting_index = step_index + 1\n",
        "#         obs, info = env_for_video.reset()\n",
        "\n",
        "# env_for_video.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congratulations! Finally, copy the `predict_probs`, `get_cumulative_rewards` and `get_loss` to the template and submit them to the Contest.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus part (no points, just for the interested ones)\n",
        "\n",
        "Try solving the `Acrobot-v1` environment. It is more complex than regular `CartPole-v1`, so the default Policy Gradient (REINFORCE) algorithm might not work. Maybe the baseline idea could help...\n",
        "\n",
        "![Acrobot](https://gymnasium.farama.org/_images/acrobot.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your brave and victorious code here."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
